{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0368139-f63a-459b-8a17-c9d168bdff8b",
   "metadata": {},
   "source": [
    "# RL for Maximizing Q_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252e2ce6-689a-4550-a5b0-ac0a910e9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gymtorax\n",
    "import gymnasium as gym\n",
    "import gymtorax.action_handler as ah\n",
    "import gymtorax.observation_handler as oh\n",
    "from gymtorax.envs.base_env import BaseEnv\n",
    "from gymtorax import rewards as reward\n",
    "import time\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e628f617-46a5-4b81-a954-cfeff61d77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"gymtorax\").setLevel(logging.ERROR)\n",
    "logging.getLogger('gymnasium').setLevel(logging.ERROR)\n",
    "logging.getLogger('gym').setLevel(logging.ERROR)\n",
    "logging.getLogger(\"gymtorax.action_handler\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bd386-b3bb-4abc-b5e2-8c178e7536d2",
   "metadata": {},
   "source": [
    "## Import State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d225881-5df6-4364-83ae-4b1ab19c7ade",
   "metadata": {},
   "source": [
    "Our RL agent will only have partial observability into the total state space provided by the TORAX simulator. Here, we import the states we want our agent to be able to observe from a CSV file listing all of the state parameters in the state space. We will then integrate the states that we wish for our agent to observe into our training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c40de2-722e-49ca-b070-9551366e3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS_PROFILES = ['T_e', 'T_i', 'n_e', 'psi', 'psi_norm', 'spr', 'R_in', 'R_out', 'delta', 'delta_lower', 'delta_upper', 'elongation', 'magnetic_shear', 'q', 'p_ohmic_e']\n",
      "OBS_SCALARS = ['H20', 'H89P', 'H97L', 'H98', 'Q_fusion', 'beta_N', 'beta_pol', 'beta_tor', 'dW_thermal_dt', 'tau_E', 'E_aux', 'E_fusion', 'P_alpha_total', 'P_aux_total', 'P_cyclotron_e', 'P_SOL_total', 'B_0', 'R_major', 'a_minor', 'q95', 'q_min', 'rho_q_min', 'S_gas_puff', 'S_generic_particle', 'S_pellet', 'S_total', 'W_pol', 'W_thermal_e', 'W_thermal_i', 'W_thermal_total', 'fgw_n_e_line_avg', 'fgw_n_e_volume_avg', 'n_e_line_avg', 'n_e_volume_avg', 'A_i', 'A_impurity', 'I_aux_generic', 'I_bootstrap', 'I_ecrh', 'P_icrh_total', 'Phi_b', 'Phi_b_dot', 'li3', 'v_loop_lcfs']\n"
     ]
    }
   ],
   "source": [
    "# Extract states that are meant to be observable to the RL agent\n",
    "state_space_csv = \"Gym_TORAX_IterHybrid-v0_env - State Space.csv\"\n",
    "df = pd.read_csv(state_space_csv)\n",
    "observable_df = df[df[\"OBSERVABLE?\"] == \"Yes\"][[\"NAME\", \"TYPE\"]].dropna()\n",
    "\n",
    "# Profile (vector) variables\n",
    "OBS_PROFILES = (\n",
    "    observable_df[observable_df[\"TYPE\"] == \"vector\"][\"NAME\"]\n",
    "    .dropna()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Scalar variables\n",
    "OBS_SCALARS = (\n",
    "    observable_df[observable_df[\"TYPE\"] != \"vector\"][\"NAME\"]\n",
    "    .dropna()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"OBS_PROFILES =\", OBS_PROFILES)\n",
    "print(\"OBS_SCALARS =\", OBS_SCALARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3725cad-e0eb-4982-873c-3d4b95b2c192",
   "metadata": {},
   "source": [
    "## Define Training Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a80ca-4ab0-4d0e-ae67-f28ad8bd20cb",
   "metadata": {},
   "source": [
    "Here we implement the Gymnasium training environment. The environment includes the state space, the observation space (a subset of the state space consisting of the state parameters that the RL agent can observe), the action space, and the reward function. We have modified Gym-TORAX's existing base ITER hybrid environment. Our changes to the existing environment are:\n",
    "\n",
    "1) We have hidden the majority of the state parameters from the RL agent. Only the parameters imported from the CSV file are visible to the agent at any time step.\n",
    "2) We have modified the reward function to suit our custom objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd52d04-7dd2-408a-aa73-32280d971ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Config for ITER hybrid scenario based parameters with nonlinear solver.\n",
    "\n",
    "ITER hybrid scenario based (roughly) on van Mulders Nucl. Fusion 2021.\n",
    "With Newton-Raphson solver and adaptive timestep (backtracking)\n",
    "\"\"\"\n",
    "\n",
    "_NBI_W_TO_MA = 1 / 16e6  # rough estimate of NBI heating power to current drive\n",
    "W_to_Ne_ratio = 0\n",
    "\n",
    "# No NBI during rampup. Rampup all NBI power between 99-100 seconds\n",
    "nbi_times = np.array([0, 99, 100])\n",
    "nbi_powers = np.array([0, 0, 33e6])\n",
    "nbi_cd = nbi_powers * _NBI_W_TO_MA\n",
    "\n",
    "# Gaussian prescription of \"NBI\" deposition profiles and fractional deposition\n",
    "r_nbi = 0.25\n",
    "w_nbi = 0.25\n",
    "el_heat_fraction = 0.66\n",
    "\n",
    "# No ECCD power for this config (but kept here for future flexibility)\n",
    "eccd_power = {0: 0, 99: 0, 100: 20.0e6}\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"plasma_composition\": {\n",
    "        \"main_ion\": {\"D\": 0.5, \"T\": 0.5},  # (bundled isotope average)\n",
    "        \"impurity\": {\"Ne\": 1 - W_to_Ne_ratio, \"W\": W_to_Ne_ratio},\n",
    "        \"Z_eff\": {0.0: {0.0: 2.0, 1.0: 2.0}},  # sets impurity densities\n",
    "    },\n",
    "    \"profile_conditions\": {\n",
    "        \"Ip\": {0: 3e6, 100: 12.5e6},  # total plasma current in MA\n",
    "        \"T_i\": {0.0: {0.0: 6.0, 1.0: 0.2}},  # T_i initial condition\n",
    "        \"T_i_right_bc\": 0.2,  # T_i boundary condition\n",
    "        \"T_e\": {0.0: {0.0: 6.0, 1.0: 0.2}},  # T_e initial condition\n",
    "        \"T_e_right_bc\": 0.2,  # T_e boundary condition\n",
    "        \"n_e_right_bc_is_fGW\": True,\n",
    "        \"n_e_right_bc\": {0: 0.35, 100: 0.35},  # n_e boundary condition\n",
    "        # set initial condition density according to Greenwald fraction.\n",
    "        \"nbar\": 0.85,  # line average density for initial condition\n",
    "        \"n_e\": {0: {0.0: 1.3, 1.0: 1.0}},  # Initial electron density profile\n",
    "        \"normalize_n_e_to_nbar\": True,  # normalize initial n_e to nbar\n",
    "        \"n_e_nbar_is_fGW\": True,  # nbar is in units for greenwald fraction\n",
    "        \"initial_psi_from_j\": True,  # initial psi from current formula\n",
    "        \"initial_j_is_total_current\": True,  # only ohmic current on init\n",
    "        \"current_profile_nu\": 2,  # exponent in initial current formula\n",
    "    },\n",
    "    \"numerics\": {\n",
    "        \"t_final\": 150,  # length of simulation time in seconds\n",
    "        \"fixed_dt\": 1,  # fixed timestep\n",
    "        \"evolve_ion_heat\": True,  # solve ion heat equation\n",
    "        \"evolve_electron_heat\": True,  # solve electron heat equation\n",
    "        \"evolve_current\": True,  # solve current equation\n",
    "        \"evolve_density\": True,  # solve density equation\n",
    "    },\n",
    "    \"geometry\": {\n",
    "        \"geometry_type\": \"chease\",\n",
    "        \"geometry_file\": \"ITER_hybrid_citrin_equil_cheasedata.mat2cols\",\n",
    "        \"Ip_from_parameters\": True,\n",
    "        \"R_major\": 6.2,  # major radius (R) in meters\n",
    "        \"a_minor\": 2.0,  # minor radius (a) in meters\n",
    "        \"B_0\": 5.3,  # Toroidal magnetic field on axis [T]\n",
    "    },\n",
    "    \"sources\": {\n",
    "        # Current sources (for psi equation)\n",
    "        \"ecrh\": {  # ECRH/ECCD (with Lin-Liu)\n",
    "            \"gaussian_width\": 0.05,\n",
    "            \"gaussian_location\": 0.35,\n",
    "            \"P_total\": eccd_power,\n",
    "        },\n",
    "        \"generic_heat\": {  # Proxy for NBI heat source\n",
    "            \"gaussian_location\": r_nbi,  # Gaussian location in normalized coordinates\n",
    "            \"gaussian_width\": w_nbi,  # Gaussian width in normalized coordinates\n",
    "            \"P_total\": (nbi_times, nbi_powers),  # Total heating power\n",
    "            # electron heating fraction r\n",
    "            \"electron_heat_fraction\": el_heat_fraction,\n",
    "        },\n",
    "        \"generic_current\": {  # Proxy for NBI current source\n",
    "            \"use_absolute_current\": True,  # I_generic is total external current\n",
    "            \"gaussian_width\": w_nbi,\n",
    "            \"gaussian_location\": r_nbi,\n",
    "            \"I_generic\": (nbi_times, nbi_cd),\n",
    "        },\n",
    "        \"fusion\": {},  # fusion power\n",
    "        \"ei_exchange\": {},  # equipartition\n",
    "        \"ohmic\": {},  # ohmic power\n",
    "        \"cyclotron_radiation\": {},  # cyclotron radiation\n",
    "        \"impurity_radiation\": {  # impurity radiation + bremsstrahlung\n",
    "            \"model_name\": \"mavrin_fit\",\n",
    "            \"radiation_multiplier\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"neoclassical\": {\n",
    "        \"bootstrap_current\": {\n",
    "            \"bootstrap_multiplier\": 1.0,\n",
    "        },\n",
    "    },\n",
    "    \"pedestal\": {\n",
    "        \"model_name\": \"set_T_ped_n_ped\",\n",
    "        # use internal boundary condition model (for H-mode and L-mode)\n",
    "        \"set_pedestal\": True,\n",
    "        \"T_i_ped\": {0: 0.5, 100: 0.5, 105: 3.0},\n",
    "        \"T_e_ped\": {0: 0.5, 100: 0.5, 105: 3.0},\n",
    "        \"n_e_ped_is_fGW\": True,\n",
    "        \"n_e_ped\": 0.85,  # pedestal top n_e in units of fGW\n",
    "        \"rho_norm_ped_top\": 0.95,  # set ped top location in normalized radius\n",
    "    },\n",
    "    \"transport\": {\n",
    "        \"model_name\": \"qlknn\",  # Using QLKNN_7_11 default\n",
    "        # set inner core transport coefficients (ad-hoc MHD/EM transport)\n",
    "        \"apply_inner_patch\": True,\n",
    "        \"D_e_inner\": 0.15,\n",
    "        \"V_e_inner\": 0.0,\n",
    "        \"chi_i_inner\": 0.3,\n",
    "        \"chi_e_inner\": 0.3,\n",
    "        \"rho_inner\": 0.1,  # radius below which patch transport is applied\n",
    "        # set outer core transport coefficients (L-mode near edge region)\n",
    "        \"apply_outer_patch\": True,\n",
    "        \"D_e_outer\": 0.1,\n",
    "        \"V_e_outer\": 0.0,\n",
    "        \"chi_i_outer\": 2.0,\n",
    "        \"chi_e_outer\": 2.0,\n",
    "        \"rho_outer\": 0.95,  # radius above which patch transport is applied\n",
    "        # allowed chi and diffusivity bounds\n",
    "        \"chi_min\": 0.05,  # minimum chi\n",
    "        \"chi_max\": 100,  # maximum chi (can be helpful for stability)\n",
    "        \"D_e_min\": 0.05,  # minimum electron diffusivity\n",
    "        \"D_e_max\": 50,  # maximum electron diffusivity\n",
    "        \"V_e_min\": -10,  # minimum electron convection\n",
    "        \"V_e_max\": 10,  # minimum electron convection\n",
    "        \"smoothing_width\": 0.1,\n",
    "        \"DV_effective\": True,\n",
    "        \"include_ITG\": True,  # to toggle ITG modes on or off\n",
    "        \"include_TEM\": True,  # to toggle TEM modes on or off\n",
    "        \"include_ETG\": True,  # to toggle ETG modes on or off\n",
    "        \"avoid_big_negative_s\": False,\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"solver_type\": \"linear\",  # linear solver with picard iteration\n",
    "        \"use_predictor_corrector\": True,  # for linear solver\n",
    "        \"n_corrector_steps\": 10,  # for linear solver\n",
    "        \"chi_pereverzev\": 30,\n",
    "        \"D_pereverzev\": 15,\n",
    "        \"use_pereverzev\": True,\n",
    "        #        'log_iterations': False,\n",
    "    },\n",
    "    \"time_step_calculator\": {\n",
    "        \"calculator_type\": \"fixed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class ReducedObservation(oh.Observation):\n",
    "    def __init__(self, profiles=None, scalars=None, custom_bounds_file=None):\n",
    "        variables = {\n",
    "            \"profiles\": profiles if profiles is not None else [],\n",
    "            \"scalars\": scalars if scalars is not None else [],\n",
    "        }\n",
    "\n",
    "        super().__init__(\n",
    "            variables=variables,\n",
    "            custom_bounds_filename=custom_bounds_file\n",
    "        )\n",
    "\n",
    "\n",
    "class IterHybridEnvPartialObservability(BaseEnv):\n",
    "\n",
    "    def __init__(self, render_mode=None, **kwargs):\n",
    "\n",
    "        # Set environment-specific defaults\n",
    "        kwargs.setdefault(\"log_level\", \"warning\")\n",
    "        kwargs.setdefault(\"plot_config\", \"default\")\n",
    "\n",
    "        super().__init__(render_mode=render_mode, **kwargs)\n",
    "\n",
    "    def _define_action_space(self):\n",
    "        actions = [\n",
    "            ah.IpAction(\n",
    "                max=[15e6],  # 15 MA max plasma current\n",
    "                ramp_rate=[0.2e6],\n",
    "            ),  # 0.2 MA/s ramp rate limit\n",
    "            ah.NbiAction(\n",
    "                max=[33e6, 1.0, 1.0],  # 33 MW max NBI power\n",
    "            ),\n",
    "            ah.EcrhAction(\n",
    "                max=[20e6, 1.0, 1.0],  # 20 MW max ECRH power\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def _define_observation_space(self):\n",
    "        # return AllObservation(custom_bounds_file=\"gymtorax/envs/iter_hybrid.json\n",
    "        return ReducedObservation(\n",
    "            profiles=OBS_PROFILES,\n",
    "            scalars=OBS_SCALARS,\n",
    "            custom_bounds_file=None\n",
    "        )\n",
    "\n",
    "    def _get_torax_config(self):\n",
    "        return {\n",
    "            \"config\": CONFIG,\n",
    "            \"discretization\": \"fixed\",\n",
    "            \"ratio_a_sim\": 1,\n",
    "        }\n",
    "\n",
    "    def _compute_reward(self, state, next_state, action):\n",
    "        weight_list = [1, 1, 1, 1, 1]\n",
    "\n",
    "        # Rewards are only provided for fusion gain when the plasma is in H-mode\n",
    "        def _is_H_mode():\n",
    "            if (\n",
    "                next_state[\"profiles\"][\"T_e\"][0] > 10\n",
    "                and next_state[\"profiles\"][\"T_i\"][0] > 10\n",
    "            ):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        # Fusion gain Q reward: r_fusion = Q / 10\n",
    "        def _r_fusion_gain():\n",
    "            fusion_gain = (\n",
    "                reward.get_fusion_gain(next_state) / 10\n",
    "            )  # Normalize with ITER target\n",
    "            if _is_H_mode():\n",
    "                return fusion_gain\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Confinement quality H98 reward: reward H98 value up to 1 (higher H98 value = better confinement)\n",
    "        def _r_h98():\n",
    "            h98 = reward.get_h98(next_state)\n",
    "            if _is_H_mode():\n",
    "                if h98 <= 1:\n",
    "                    return h98\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Minimum safety factor reward: reward q_min value up to 1 (q_min < 1 leads to disruptions)\n",
    "        def _r_q_min():\n",
    "            q_min = reward.get_q_min(next_state)\n",
    "            if q_min <= 1:\n",
    "                return q_min\n",
    "            elif q_min > 1:\n",
    "                return 1\n",
    "\n",
    "        # Edge safety factor reward: reward q_95 value / 3 value up to 1\n",
    "        def _r_q_95():\n",
    "            q_95 = reward.get_q95(next_state)\n",
    "            if q_95 / 3 <= 1:\n",
    "                return q_95 / 3\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        # Greenwald fraction reward: reward fgw value < 0.9 with 1, 0.9 - 1 with a small reward, > 1 with 0\n",
    "        def _r_greenwald():\n",
    "            fgw = float(next_state[\"scalars\"][\"fgw_n_e_line_avg\"][0])  # Need to custom define this reward\n",
    "        \n",
    "            if fgw <= 0.9:\n",
    "                return 1\n",
    "            elif fgw <= 1:\n",
    "                return 1 - (fgw - 0.9) / 0.1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Calculate individual reward components\n",
    "        r_fusion_gain = weight_list[0] * _r_fusion_gain() / 50\n",
    "        r_h98 = weight_list[1] * _r_h98() / 50\n",
    "        r_q_min = weight_list[2] * _r_q_min() / 150\n",
    "        r_q_95 = weight_list[3] * _r_q_95() / 150\n",
    "        r_greenwald = weight_list[4] * _r_greenwald() / 150\n",
    "\n",
    "        total_reward = r_fusion_gain + r_h98 + r_q_min + r_q_95 + r_greenwald\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e6847-09df-4b22-b083-d0185f5ec30a",
   "metadata": {},
   "source": [
    "## Implement PPO Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0fa29-719a-443d-86f8-9e6cf179f8fa",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187e9319-df21-46ce-9bd5-7efcc4cd488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "class FlattenToraxObservation(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Convert TORAX observation dict:\n",
    "    \n",
    "        {\n",
    "            \"profiles\": {var: np.array(shape=(n_points,))},\n",
    "            \"scalars\":  {var: np.array([value])}\n",
    "        }\n",
    "\n",
    "    into one flat 1D vector Box for use with Stable-Baselines3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        obs_space = env.observation_space\n",
    "        assert isinstance(obs_space, spaces.Dict), \\\n",
    "            \"Expected Dict observation space from TORAX.\"\n",
    "\n",
    "        # Store key order so flattening is consistent across time steps\n",
    "        self.profile_keys = list(obs_space[\"profiles\"].spaces.keys())\n",
    "        self.scalar_keys = list(obs_space[\"scalars\"].spaces.keys())\n",
    "\n",
    "        # Compute total dimensionality\n",
    "        dim = 0\n",
    "\n",
    "        # Profiles (vectors)\n",
    "        for key in self.profile_keys:\n",
    "            box = obs_space[\"profiles\"].spaces[key]\n",
    "            dim += int(np.prod(box.shape))\n",
    "\n",
    "        # Scalars (1D size=1)\n",
    "        for key in self.scalar_keys:\n",
    "            box = obs_space[\"scalars\"].spaces[key]\n",
    "            dim += int(np.prod(box.shape))\n",
    "\n",
    "        # Define new flattened observation space\n",
    "        low = -np.inf * np.ones(dim, dtype=np.float32)\n",
    "        high = np.inf * np.ones(dim, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"Flatten TORAX nested dict observation into a 1D vector.\"\"\"\n",
    "        parts = []\n",
    "\n",
    "        # Profiles\n",
    "        for key in self.profile_keys:\n",
    "            parts.append(np.asarray(obs[\"profiles\"][key]).ravel())\n",
    "\n",
    "        # Scalars\n",
    "        for key in self.scalar_keys:\n",
    "            parts.append(np.asarray(obs[\"scalars\"][key]).ravel())\n",
    "\n",
    "        flat = np.concatenate(parts).astype(np.float32)\n",
    "        return flat\n",
    "\n",
    "\n",
    "class FlattenToraxAction(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Convert TORAX action Dict {\n",
    "        'Ip': Box(1,)\n",
    "        'NBI': Box(3,)\n",
    "        'ECRH': Box(3,)\n",
    "    }\n",
    "    into a single 1D Box for SB3, and unflatten on env.step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        act_space = env.action_space\n",
    "        assert isinstance(act_space, spaces.Dict), \\\n",
    "            \"Expected Dict action space from TORAX\"\n",
    "\n",
    "        self.keys = list(act_space.spaces.keys())\n",
    "\n",
    "        # Compute flattened dimension\n",
    "        dims = []\n",
    "        for k in self.keys:\n",
    "            dims.append(int(np.prod(act_space[k].shape)))\n",
    "\n",
    "        self.key_dims = dims\n",
    "        total_dim = sum(dims)\n",
    "\n",
    "        # Create the new flattened Box action space\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for k in self.keys:\n",
    "            box = act_space[k]\n",
    "            lows.append(box.low.flatten())\n",
    "            highs.append(box.high.flatten())\n",
    "\n",
    "        lows = np.concatenate(lows).astype(np.float32)\n",
    "        highs = np.concatenate(highs).astype(np.float32)\n",
    "\n",
    "        self.action_space = spaces.Box(low=lows, high=highs, dtype=np.float32)\n",
    "\n",
    "    def action(self, flat_action):\n",
    "        \"\"\"\n",
    "        Convert flat action vector back into TORAX dict for env.step().\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        idx = 0\n",
    "        for k, dim in zip(self.keys, self.key_dims):\n",
    "            out[k] = flat_action[idx: idx + dim]\n",
    "            idx += dim\n",
    "        return out\n",
    "\n",
    "\n",
    "class ActionRescaleWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Maps actions from [-1, 1] to the TORAX physical action ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.low = env.action_space.low\n",
    "        self.high = env.action_space.high\n",
    "\n",
    "        # New action space presented to the agent\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=self.low.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def action(self, normalized_action):\n",
    "        # Convert from [-1,1] to [low, high]\n",
    "        return self.low + (normalized_action + 1.0) * 0.5 * (self.high - self.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4ad2e7-0865-4942-98d2-68f671f385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = IterHybridEnvPartialObservability()\n",
    "# env = FlattenToraxObservation(env)\n",
    "# env = FlattenToraxAction(env)\n",
    "# env = ActionRescaleWrapper(env)\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# print(\"obs shape:\", obs.shape)\n",
    "# print(\"action space:\", env.action_space)\n",
    "# print(\"observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac3379-1d78-477d-a91b-1157b4490867",
   "metadata": {},
   "source": [
    "### PPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843c3c0c-18bd-4a6e-9bc2-e8b868fe286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02676d48-b0ce-4ee0-8607-c4e27ca585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factory\n",
    "def make_env(seed: int | None = None) -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create a single wrapped ITER hybrid environment ready for SB3.\n",
    "    - partial observability\n",
    "    - flattened observations\n",
    "    - flattened actions\n",
    "    \"\"\"\n",
    "    env = IterHybridEnvPartialObservability(render_mode=None, store_history=False)\n",
    "\n",
    "    # Flatten and rescale observation and action dictionaries\n",
    "    env = FlattenToraxObservation(env)\n",
    "    env = FlattenToraxAction(env)\n",
    "    env = ActionRescaleWrapper(env)\n",
    "\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e873ea-dc52-417f-9a3e-dc2efaef1617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading model from logs/ppo_iter_2025-11-25_18-31-06/checkpoints/ppo_ckpt_5000_steps.zip\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs\\ppo_iter_2025-11-25_18-53-42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3791732af5f04b45a60c45804f7ff724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | 4.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 5        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 7048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 9096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017282587 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.89       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0247      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 0.0649      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 973         |\n",
      "|    total_timesteps      | 11144       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015319216 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.9        |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.0756      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1283        |\n",
      "|    total_timesteps      | 13192       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016982459 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.89       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00537     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.0558      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1600        |\n",
      "|    total_timesteps      | 15240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010916208 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.89       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1914        |\n",
      "|    total_timesteps      | 17288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015335331 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.92       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00801     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0077     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.085       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 2257        |\n",
      "|    total_timesteps      | 19336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019088833 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.92       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.081       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 151        |\n",
      "|    ep_rew_mean          | 4.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 2580       |\n",
      "|    total_timesteps      | 21384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01686955 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.89      |\n",
      "|    explained_variance   | 0.357      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.099      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00474   |\n",
      "|    std                  | 0.994      |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 151        |\n",
      "|    ep_rew_mean          | 4.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 2910       |\n",
      "|    total_timesteps      | 23432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01848336 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.9       |\n",
      "|    explained_variance   | 0.338      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0792     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.996      |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 4.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 3249        |\n",
      "|    total_timesteps      | 25480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017976291 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.92       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 5.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 3596        |\n",
      "|    total_timesteps      | 27528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017798305 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.93       |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00452    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 151        |\n",
      "|    ep_rew_mean          | 5.3        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 3925       |\n",
      "|    total_timesteps      | 29576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01573974 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.93      |\n",
      "|    explained_variance   | 0.144      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0724     |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00646   |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=4.88 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=30000, episode_reward=4.88 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 151.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 151.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 151         |\n",
      "|    mean_reward          | 4.88        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017279124 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.93       |\n",
      "|    explained_variance   | 0.0785      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.137       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | 5.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 6        |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 4352     |\n",
      "|    total_timesteps | 31624    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 5.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 4710        |\n",
      "|    total_timesteps      | 33672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021253224 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.92       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 6.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 5091        |\n",
      "|    total_timesteps      | 35720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014878422 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.9        |\n",
      "|    explained_variance   | -0.0129     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.398       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 6.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 5428        |\n",
      "|    total_timesteps      | 37768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019173581 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.88       |\n",
      "|    explained_variance   | 0.0243      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 0.432       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 6.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 5748        |\n",
      "|    total_timesteps      | 39816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013067259 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.93       |\n",
      "|    explained_variance   | 0.0463      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.273       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.406       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 7.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 6076        |\n",
      "|    total_timesteps      | 41864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015904112 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.97       |\n",
      "|    explained_variance   | 0.00594     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.507       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 7.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 6412        |\n",
      "|    total_timesteps      | 43912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015855582 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.97       |\n",
      "|    explained_variance   | 0.0288      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.312       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00973    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.806       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 7.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 6724        |\n",
      "|    total_timesteps      | 45960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022776335 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.96       |\n",
      "|    explained_variance   | 0.0469      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.572       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 151        |\n",
      "|    ep_rew_mean          | 7.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 7040       |\n",
      "|    total_timesteps      | 48008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02355237 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.93      |\n",
      "|    explained_variance   | 0.0536     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.32       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.00432   |\n",
      "|    std                  | 0.996      |\n",
      "|    value_loss           | 0.679      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 8.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 7372        |\n",
      "|    total_timesteps      | 50056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020652233 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.85       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.521       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 8.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 7690        |\n",
      "|    total_timesteps      | 52104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025760043 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.81       |\n",
      "|    explained_variance   | 0.0292      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.26        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.615       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 8.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 8018        |\n",
      "|    total_timesteps      | 54152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016598985 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.81       |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.176       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.655       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=55000, episode_reward=6.67 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=55000, episode_reward=6.67 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 151.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 151.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 151         |\n",
      "|    mean_reward          | 6.67        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016209096 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.8        |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.21        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.706       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Total timesteps ‚Äì you can start smaller for a smoke test (e.g. 50_000)\u001b[39;00m\n\u001b[32m     74\u001b[39m total_timesteps = \u001b[32m200_000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[32m     84\u001b[39m model.save(os.path.join(log_dir, \u001b[33m\"\u001b[39m\u001b[33mppo_iter_final\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\gymnasium\\core.py:636\u001b[39m, in \u001b[36mActionWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    633\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    634\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    635\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\gymnasium\\core.py:636\u001b[39m, in \u001b[36mActionWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    633\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    634\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    635\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\gymnasium\\core.py:560\u001b[39m, in \u001b[36mObservationWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    558\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    559\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observation(observation), reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\gymtorax\\envs\\base_env.py:340\u001b[39m, in \u001b[36mBaseEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    337\u001b[39m info = {}\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# Capture current state before applying action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorax_app\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Apply action by updating TORAX configuration parameters\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m.torax_app.update_config(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\gymtorax\\torax_wrapper\\torax_app.py:514\u001b[39m, in \u001b[36mToraxApp.get_state_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSimulation state has not been computed yet.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulation_output_to_xr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\torax\\_src\\output_tools\\output.py:362\u001b[39m, in \u001b[36mStateHistory.simulation_output_to_xr\u001b[39m\u001b[34m(self, file_restart)\u001b[39m\n\u001b[32m    349\u001b[39m coords = {\n\u001b[32m    350\u001b[39m     TIME: time,\n\u001b[32m    351\u001b[39m     RHO_FACE_NORM: rho_face_norm,\n\u001b[32m    352\u001b[39m     RHO_CELL_NORM: rho_cell_norm,\n\u001b[32m    353\u001b[39m     RHO_NORM: rho_norm,\n\u001b[32m    354\u001b[39m }\n\u001b[32m    356\u001b[39m \u001b[38;5;66;03m# Update dict with flattened StateHistory dataclass containers\u001b[39;00m\n\u001b[32m    357\u001b[39m all_dicts = [\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_core_profiles(),\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_core_transport(),\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_core_sources(),\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_post_processed_outputs(),\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    363\u001b[39m ]\n\u001b[32m    364\u001b[39m flat_dict = {}\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m itertools.chain(*(d.items() \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m all_dicts)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\torax\\_src\\output_tools\\output.py:618\u001b[39m, in \u001b[36mStateHistory._save_geometry\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save geometry to a dict. We skip over hires and non-array quantities.\"\"\"\u001b[39;00m\n\u001b[32m    617\u001b[39m xr_dict = {}\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m geometry_attributes = \u001b[43mdataclasses\u001b[49m\u001b[43m.\u001b[49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stacked_geometry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;66;03m# Get the variables from dataclass fields.\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field_name, data \u001b[38;5;129;01min\u001b[39;00m geometry_attributes.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\dataclasses.py:1359\u001b[39m, in \u001b[36masdict\u001b[39m\u001b[34m(obj, dict_factory)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dataclass_instance(obj):\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33masdict() should be called on dataclass instances\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\dataclasses.py:1370\u001b[39m, in \u001b[36m_asdict_inner\u001b[39m\u001b[34m(obj, dict_factory)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj_type, _FIELDS):\n\u001b[32m   1367\u001b[39m     \u001b[38;5;66;03m# dataclass instance: fast path for the common case\u001b[39;00m\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dict_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m   1369\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m             f.name: \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1371\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields(obj)\n\u001b[32m   1372\u001b[39m         }\n\u001b[32m   1373\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dict_factory([\n\u001b[32m   1375\u001b[39m             (f.name, _asdict_inner(\u001b[38;5;28mgetattr\u001b[39m(obj, f.name), dict_factory))\n\u001b[32m   1376\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields(obj)\n\u001b[32m   1377\u001b[39m         ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\dataclasses.py:1427\u001b[39m, in \u001b[36m_asdict_inner\u001b[39m\u001b[34m(obj, dict_factory)\u001b[39m\n\u001b[32m   1425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj_type(_asdict_inner(v, dict_factory) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj)\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\copy.py:144\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    142\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:626\u001b[39m, in \u001b[36m_deepcopy\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array, memo: Any) -> Array:\n\u001b[32m    625\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m memo  \u001b[38;5;66;03m# unused\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:170\u001b[39m, in \u001b[36m_copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array) -> Array:\n\u001b[32m    166\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Return a copy of the array.\u001b[39;00m\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m \u001b[33;03m  Refer to :func:`jax.numpy.copy` for the full documentation.\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:5401\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(a, order)\u001b[39m\n\u001b[32m   5357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a copy of the array.\u001b[39;00m\n\u001b[32m   5358\u001b[39m \n\u001b[32m   5359\u001b[39m \u001b[33;03mJAX implementation of :func:`numpy.copy`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5398\u001b[39m \u001b[33;03m  [0 1 2 3]\u001b[39;00m\n\u001b[32m   5399\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5400\u001b[39m util.check_arraylike(\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m, a)\n\u001b[32m-> \u001b[39m\u001b[32m5401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\array_constructors.py:263\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(object, dtype, copy, order, ndmin, device, out_sharding)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, Array):\n\u001b[32m    262\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mobject\u001b[39m.aval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m   out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_array_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    265\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:8561\u001b[39m, in \u001b[36m_array_copy\u001b[39m\u001b[34m(arr)\u001b[39m\n\u001b[32m   8560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_copy\u001b[39m(arr: ArrayLike) -> Array:\n\u001b[32m-> \u001b[39m\u001b[32m8561\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\core.py:632\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    631\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\core.py:648\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    646\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    650\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\core.py:660\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    659\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt apply typeof to args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\core.py:1189\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1187\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1188\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:8602\u001b[39m, in \u001b[36m_copy_impl\u001b[39m\u001b[34m(prim, *args, **kwargs)\u001b[39m\n\u001b[32m   8600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch.apply_primitive(prim, *args, **kwargs)\n\u001b[32m   8601\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m8602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\jax\\_src\\dispatch.py:94\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     92\u001b[39m prev = config.disable_jit.swap_local(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m   outs = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     96\u001b[39m   config.disable_jit.set_local(prev)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Make training and eval environment\n",
    "train_env = make_env(seed=0)\n",
    "eval_env = make_env(seed=1)\n",
    "\n",
    "# Logging + checkpoints\n",
    "run_name = datetime.now().strftime(\"ppo_iter_%Y-%m-%d_%H-%M-%S\")\n",
    "log_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "tensorboard_log = os.path.join(log_dir, \"tb\")\n",
    "\n",
    "# Resume training from checkpoint\n",
    "# resume_checkpoint = None\n",
    "resume_checkpoint = \"logs/ppo_iter_2025-11-25_18-31-06/checkpoints/ppo_ckpt_5000_steps.zip\"\n",
    "\n",
    "if resume_checkpoint is not None:\n",
    "    print(f\"üîÅ Loading model from {resume_checkpoint}\")\n",
    "    model = PPO.load(resume_checkpoint, env=train_env)\n",
    "    continue_training = True\n",
    "else:\n",
    "    print(\"üÜï Starting NEW PPO training.\")\n",
    "    continue_training = False\n",
    "\n",
    "# PPO moded trained from scratch\n",
    "if not continue_training:\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        learning_rate=3e-4,\n",
    "        ent_coef=0.0,\n",
    "        clip_range=0.2,\n",
    "        gae_lambda=0.95,\n",
    "        n_epochs=10,\n",
    "        tensorboard_log=tensorboard_log,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    )\n",
    "\n",
    "# Resuming training\n",
    "else:\n",
    "    model.set_env(train_env)\n",
    "    model.tensorboard_log = tensorboard_log\n",
    "\n",
    "# Configure SB3 logger\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Save a checkpoint every N steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=5000,  # env steps\n",
    "    save_path=checkpoint_dir,\n",
    "    name_prefix=\"ppo_ckpt\",\n",
    ")\n",
    "\n",
    "# Evaluate periodically on a separate environment\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(log_dir, \"best_model\"),\n",
    "    log_path=os.path.join(log_dir, \"eval\"),\n",
    "    eval_freq=25_000,  # How often to run eval (steps)\n",
    "    n_eval_episodes=3,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "# Total timesteps ‚Äì you can start smaller for a smoke test (e.g. 50_000)\n",
    "total_timesteps = 200_000\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=[checkpoint_callback, eval_callback],\n",
    "    progress_bar=True,\n",
    "    reset_num_timesteps=not continue_training,\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(os.path.join(log_dir, \"ppo_iter_final\"))\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7be3a-df7b-4de3-b2b8-1ed0f672dc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
